{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Ilya Nikitin CL HW Torch Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVJsnyA5STzn"
      },
      "source": [
        "# Задание"
      ],
      "id": "bVJsnyA5STzn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HELKaMscSNrQ"
      },
      "source": [
        "Нужно какое-то время пообучать трансформер для машинного перевода (те же данные что и на семинаре) и, используя, получившуюся модель перевести какой-то достаточно длинный текст на английском языке (например, какую-то новость). При этом нужно переделать функцию translate так, чтобы она обрабатывала не каждый текст по отдельности, а сразу целиком (или батчами если вы возьмете совсем длинный текст). На предложения текст можно поделить внутри или снаружи функции."
      ],
      "id": "HELKaMscSNrQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d650e9eb",
        "scrolled": true
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install tokenizers matplotlib sklearn"
      ],
      "id": "d650e9eb",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e384bd5c"
      },
      "source": [
        "# Транформеры для решения seq2seq задач"
      ],
      "id": "e384bd5c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a69baacc"
      },
      "source": [
        "Seq2seq - наверное самая общая формальная постановка задачи в NLP. Нужно из произвольной последовательности получить какую-то другую последовательность. И в отличие от разметки последовательности (sequence labelling) не требуется, чтобы обе последовательности совпадали по длине. Даже стандартную задачу классификации можно решать как seq2seq - можно рассматривать метку класса как последовательность длинны 1."
      ],
      "id": "a69baacc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe2a0525"
      },
      "source": [
        "А трансформеры - sota архитектура для seq2seq задач. Мы не будем подробно разбирать устройство транформеров, если вам интересно вы можете поразбираться вот с этими материалами:\n",
        "\n",
        "Оригинальная статья (сложновато) - https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/  \n",
        "https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "https://www.youtube.com/watch?v=iDulhoQ2pro\n",
        "\n",
        "https://www.youtube.com/watch?v=TQQlZhbC5ps\n",
        "\n",
        "Самый известный туториал (на торче) - https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "\n",
        "\n",
        "Трансформеры будут подробно разбираться на курсе глубокого обучения (по выбору) на втором курсе."
      ],
      "id": "fe2a0525"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46c794a6"
      },
      "source": [
        "Пока просто попробуем обучать модель на задаче машинного перевода. Для таких задач лучше всего использовать предобученные модели, но если у вас будет какая-то специфичная seq2seq задача, то имеет смысл попробовать обучить трансформер с нуля и в этой тертрадке вам нужно будет поменять только часть с загрузкой данных. "
      ],
      "id": "46c794a6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "947b3313"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "947b3313",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415f5ed2"
      },
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "# text = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ')\n",
        "# text = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ')\n",
        "# f = open('opus.en-ru-train.ru.txt', 'w')\n",
        "# f.write(text)\n",
        "# f.close()"
      ],
      "id": "415f5ed2",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIxM5SEJKm9b",
        "outputId": "b2d4fdb3-3a99-4024-d653-bdd12b0e2458"
      },
      "source": [
        "!gdown --id 1h4wmPd2A5J5gldpxjS3Oc4_JGYnEdWXn\n",
        "!gdown --id 1w3NduhIXUwlN55exo616AGVVwQ9_55u6\n",
        "!gdown --id 16hs2wBd6utkbWiLZA-AEnFwyzQE3LfW3"
      ],
      "id": "JIxM5SEJKm9b",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h4wmPd2A5J5gldpxjS3Oc4_JGYnEdWXn\n",
            "To: /content/model_tf.zip\n",
            "256MB [00:01, 135MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1w3NduhIXUwlN55exo616AGVVwQ9_55u6\n",
            "To: /content/opus.en-ru-train.en.txt\n",
            "67.8MB [00:01, 53.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16hs2wBd6utkbWiLZA-AEnFwyzQE3LfW3\n",
            "To: /content/opus.en-ru-train.ru.txt\n",
            "121MB [00:01, 88.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "018d83aa"
      },
      "source": [
        "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php (раздел с отдельными языковыми парами)"
      ],
      "id": "018d83aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e110ff04"
      },
      "source": [
        "en_sents = open('opus.en-ru-train.en.txt').read().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ').splitlines()"
      ],
      "id": "e110ff04",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c009c96e"
      },
      "source": [
        "Пример перевода с английского на русский"
      ],
      "id": "c009c96e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eb9b498",
        "outputId": "40e8b63b-1e28-475c-888e-d0a8040a1d01"
      },
      "source": [
        "en_sents[-1], ru_sents[-1]"
      ],
      "id": "0eb9b498",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('So what are you thinking?', 'Ну и что ты думаешь?')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c39921c4"
      },
      "source": [
        "Как обычно нам нужен токенизатор, а точнее даже 2, т.к. у нас два корпуса"
      ],
      "id": "c39921c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b79b4da"
      },
      "source": [
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en.txt\"], trainer=trainer_en)\n",
        "\n",
        "tokenizer_ru = Tokenizer(BPE())\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "trainer_ru = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru.txt\"], trainer=trainer_ru)"
      ],
      "id": "4b79b4da",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48780b24"
      },
      "source": [
        "### ВАЖНО!"
      ],
      "id": "48780b24"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b56d3b"
      },
      "source": [
        "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны. "
      ],
      "id": "f1b56d3b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dd90665"
      },
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_ru.save('tokenizer_ru')"
      ],
      "id": "0dd90665",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e0f7f77"
      },
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ],
      "id": "0e0f7f77",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9fecf3"
      },
      "source": [
        "Переводим текст в индексы вот таким образом. В начало добавляем токен '[CLS]', а в конец '[SEP]'. Если вспомните занятие по языковому моделированию, то там мы добавляли \"\\<start>\" и \"\\<end>\" - cls и sep по сути тоже самое. Вы поймете почему именно cls и sep, а не start и end, если подробнее поразбираетесь с устройством трансформеров"
      ],
      "id": "af9fecf3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc003758"
      },
      "source": [
        "def encode(text, tokenizer, max_len):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
      ],
      "id": "dc003758",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96920fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58985810-3f46-4a5c-e8e4-1da970259b60"
      },
      "source": [
        "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
        "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "id": "96920fdc",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cc0a376"
      },
      "source": [
        "# ограничимся длинной в 30 и 35 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
        "max_len_en, max_len_ru = 30, 35"
      ],
      "id": "5cc0a376",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fc2dae1"
      },
      "source": [
        "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
      ],
      "id": "7fc2dae1",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7655a4ea"
      },
      "source": [
        "Паддинг внутри класса для датасета. Еще обратите внимание, что тут не стоит параметр batch_first=True как раньше\n",
        "\n",
        "В торче принято, что размерность батча идет в конце и пример кода с трансформером расчитан на это. Конечно можно поменять сам код модели, но это сложнее, чем просто изменить тензор с данными."
      ],
      "id": "7655a4ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7634853b"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_en, texts_ru):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_IDX)\n",
        "        \n",
        "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
        "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_en)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[:, index]\n",
        "        ids_ru = self.texts_ru[:, index]\n",
        "\n",
        "        return ids_en, ids_ru"
      ],
      "id": "7634853b",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec889a8d"
      },
      "source": [
        "Разбиваем на трейн и тест"
      ],
      "id": "ec889a8d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c9eaf09"
      },
      "source": [
        "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
      ],
      "id": "9c9eaf09",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f0fcb7a"
      },
      "source": [
        "training_set = Dataset(X_en_train, X_ru_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True, )"
      ],
      "id": "2f0fcb7a",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d0980a4"
      },
      "source": [
        "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=200, shuffle=True)"
      ],
      "id": "6d0980a4",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32bb0e70"
      },
      "source": [
        "# Код трансформера"
      ],
      "id": "32bb0e70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54a83a16"
      },
      "source": [
        "Дальше код модели, он взят вот отсюда (с небольшими изменениями) - https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "Там есть комментарии по каждому этапу"
      ],
      "id": "54a83a16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "078605bf"
      },
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 150):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, \n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "#         print('pos inp')\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "#         print('pos dec')\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "#         print('pos out')\n",
        "        x = self.generator(outs)\n",
        "#         print('gen')\n",
        "        return x\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    \n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "id": "078605bf",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae05d7f6"
      },
      "source": [
        "Обратите внимание на то как мы подаем данные в модель"
      ],
      "id": "ae05d7f6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dedf9014"
      },
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, print_every=500):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "    \n",
        "    model.train()  \n",
        "\n",
        "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
        "        texts_en = texts_en.T.to(DEVICE) # чтобы батч был в конце\n",
        "        texts_ru = texts_ru.T.to(DEVICE) # чтобы батч был в конце\n",
        "        \n",
        "        # помимо текста в модель еще нужно передать целевую последовательность\n",
        "        # но не полную а без 1 последнего элемента\n",
        "        # а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
        "        texts_ru_input = texts_ru[:-1, :]\n",
        "        \n",
        "        \n",
        "        # в трансформерах нет циклов как в лстм \n",
        "        # каждый элемент связан с каждым через аттеншен\n",
        "        # чтобы имитировать последовательную обработку\n",
        "        # и чтобы не считать аттеншн с паддингом \n",
        "        # в трансформерах нужно считать много масок\n",
        "        # подробнее про это по ссылкам выше\n",
        "        (texts_en_mask, texts_ru_mask, \n",
        "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
        "        logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
        "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
        "        texts_ru_out = texts_ru[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        \n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "    \n",
        "    model.eval()  \n",
        "    with torch.no_grad():\n",
        "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
        "            texts_en = texts_en.T.to(DEVICE)\n",
        "            texts_ru = texts_ru.T.to(DEVICE)\n",
        "\n",
        "            texts_ru_input = texts_ru[:-1, :]\n",
        "\n",
        "            (texts_en_mask, texts_ru_mask, \n",
        "            texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
        "\n",
        "            logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
        "                           texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "\n",
        "            \n",
        "            texts_ru_out = texts_ru[1:, :]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
        "            epoch_loss.append(loss.item())\n",
        "            \n",
        "    return np.mean(epoch_loss)"
      ],
      "id": "dedf9014",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ea391c"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
        "RU_VOCAB_SIZE = tokenizer_ru.get_vocab_size()\n",
        "\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, EN_VOCAB_SIZE, RU_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "id": "c4ea391c",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5559362d"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "5559362d",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2070ab9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "5141e585-3998-4b41-ada2-34924615d00b"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
        "    \n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    losses.append(val_loss)\n",
        "        \n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))"
      ],
      "id": "2070ab9c",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 7.780176694869995;\n",
            "Loss: 7.190528983592987;\n",
            "Loss: 6.862414754231771;\n",
            "Loss: 6.625997709989548;\n",
            "Loss: 6.442283255386353;\n",
            "Loss: 6.291036547819774;\n",
            "Loss: 6.161857928139823;\n",
            "Loss: 6.048613839626312;\n",
            "Loss: 5.9464476958380805;\n",
            "First epoch - 4.88803840637207, saving model..\n",
            "Epoch: 1, Train loss: 5.899, Val loss: 4.888,            Epoch time=969.430s\n",
            "Loss: 4.938443283081055;\n",
            "Loss: 4.889075940132141;\n",
            "Loss: 4.8441872873306275;\n",
            "Loss: 4.8006130194664;\n",
            "Loss: 4.757604760169983;\n",
            "Loss: 4.715297318776448;\n",
            "Loss: 4.6767846885408675;\n",
            "Loss: 4.638347842335701;\n",
            "Loss: 4.600490137788984;\n",
            "Improved from 4.88803840637207 to 4.074344550132752, saving model..\n",
            "Epoch: 2, Train loss: 4.583, Val loss: 4.074,            Epoch time=981.786s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1e2417b56afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-5b3ce4064e87>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, print_every)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtexts_ru_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts_ru\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts_ru_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a21a9c8"
      },
      "source": [
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
        "    output_ids = [tokenizer_ru.token_to_id('[CLS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "    (texts_en_mask, texts_ru_mask, \n",
        "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                   texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_ru.token_to_id('[SEP]'), tokenizer_ru.token_to_id('[PAD]')]:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "        (texts_en_mask, texts_ru_mask, \n",
        "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "        pred = logits.argmax(2)[-1].item()\n",
        "\n",
        "    return (' '.join([tokenizer_ru.id_to_token(i).replace('##', '') for i in output_ids[1:]]))\n",
        "\n"
      ],
      "id": "4a21a9c8",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3a19c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "b3aa6d9e-deec-43b6-c262-a9c1888f170d"
      },
      "source": [
        "translate(\"Example\")"
      ],
      "id": "db3a19c3",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Добавление'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f284d0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "40cbf48f-d2e7-46f9-af80-eec02f4b6ac9"
      },
      "source": [
        "translate('Can you translate that?')"
      ],
      "id": "f284d0f1",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Вы можете , чтобы ты ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjaoL3CFUSDT"
      },
      "source": [
        "# Кастомная функция"
      ],
      "id": "KjaoL3CFUSDT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZoUEiqZQ1ZS"
      },
      "source": [
        "%%capture \n",
        "\n",
        "!pip install spacy==3.0.5\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "model_en = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "CZoUEiqZQ1ZS",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Npv6-3TbYo"
      },
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/vonCount/HSE-Computational-Linguistics-Course-2020-2021/main/sample_text.txt'\n",
        "text = requests.get(url).text"
      ],
      "id": "U2Npv6-3TbYo",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqgEtFs2UqLb",
        "outputId": "95ca9ac8-f568-4b01-e66e-cfcba4e1ae6e"
      },
      "source": [
        "print(text[:850])"
      ],
      "id": "CqgEtFs2UqLb",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Impact factor abandoned by Dutch university in hiring and promotion decisions\n",
            "Faculty and staff members at Utrecht University will be evaluated by their commitment to open science.\n",
            "A Dutch university says it is formally abandoning the impact factor — a standard measure of scientific success — in all hiring and promotion decisions. By early 2022, every department at Utrecht University in the Netherlands will judge its scholars by other standards, including their commitment to teamwork and their efforts to promote open science, says Paul Boselie, a governance researcher and the project leader for the university’s new Recognition and Rewards scheme. “Impact factors don’t really reflect the quality of an individual researcher or academic,” he says. “We have a strong belief that something has to change, and abandoning the impact factor is one \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMDjR_cCg7mH"
      },
      "source": [
        "def translate3(text):\n",
        "\n",
        "    doc = model_en(text)\n",
        "    in_list = list()\n",
        "    out_list = list()\n",
        "\n",
        "    for sent in doc.sents:\n",
        "          sent = str(sent)\n",
        "          input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(sent).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]    \n",
        "          in_list.append(input_ids)\n",
        "          out_list.append([tokenizer_ru.token_to_id('[CLS]')])\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i) for i in in_list]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i) for i in out_list]).to(DEVICE)\n",
        "   \n",
        "    (texts_en_mask, texts_ru_mask, \n",
        "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                   texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "    pred_list =[]\n",
        "    for i, logit in enumerate(torch.flatten(logits.argmax(2))):\n",
        "      pred = logit.item()\n",
        "      pred_list.append(pred)\n",
        "\n",
        "    while any([pred[-1] not in [tokenizer_ru.token_to_id('[SEP]'), tokenizer_ru.token_to_id('[PAD]')] for pred in out_list]):\n",
        "        for i in range(len(pred_list)):\n",
        "          if out_list[i][-1] in [tokenizer_ru.token_to_id('[SEP]'), tokenizer_ru.token_to_id('[PAD]')]:              \n",
        "              continue\n",
        "          else:\n",
        "              out_list[i].append(pred_list[i])\n",
        "\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(i) for i in out_list]).to(DEVICE)\n",
        "\n",
        "        (texts_en_mask, texts_ru_mask, \n",
        "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
        "                      texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
        "        pred_list =[]\n",
        "        for i, logit in enumerate(torch.flatten(logits.argmax(2)[-1])):\n",
        "          pred = logit.item()\n",
        "          pred_list.append(pred)\n",
        "        \n",
        "    translations_list = []\n",
        "    for i in out_list:\n",
        "      translations_list.append(' '.join([tokenizer_ru.id_to_token(x).replace('##', '') for x in i[1:-1]]))\n",
        "\n",
        "    return translations_list"
      ],
      "id": "zMDjR_cCg7mH",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqJ6SRn7hO2a",
        "outputId": "19bd1fe9-73f1-44d8-f1af-cd792692b616"
      },
      "source": [
        "translate3(text)"
      ],
      "id": "aqJ6SRn7hO2a",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['В связи с помощью в рамках проекта « обеспечения безопасности и укрепления потенциала в качестве решения по - прежнему не менее , а также более , что в котором говорится о , что в настоящее',\n",
              " 'В этом году и члены Совета по вопросам развития , которые будут представлены их приверженность вопросам , которые будут приняты для их мандата , а также на его пути к нему , в котором «',\n",
              " 'В .',\n",
              " 'В связи с помощью , говорит , что это предусмотрено , в результате чего бы не будет иметь возможность изменения климата , в которых будет проведено в все необходимые меры , что в настоящее время',\n",
              " 'В ходе 20 22 22 , каждый из всех сотрудников , в котором будут приняты его учреждения , которые будут приняты другие учреждения , в том числе их усилия по вопросам , включая их приверженность',\n",
              " '« В этом не то , что не было бы очень важно , чтобы обеспечить , что он был очень мощ ным и на земле , что он говорит по - английски ): « В',\n",
              " '« Мы высоко оценивает , что - то , что - то есть , что - то , что что , в то есть , что - то есть , что - то есть ,',\n",
              " '',\n",
              " 'В результате этого изменения , в результате того , чтобы в результате проведения изменения в результате и с учетом того , что в результате чего эти документы были представлены в качестве документа , которые были',\n",
              " 'В этой системы , в частности , в результате чего - то есть в случае , в результате чего вы не можете использовать более , чем в результате чего не является более чем в результате',\n",
              " 'В случае , что в качестве не менее , как правило , как правило , является результатом того , как в качестве документа - не будет способствовать « динами чных » ( на основе и',\n",
              " '« Это было очень много , что в этом случае не будет , что в том , что это было очень очень , что в то время как и я сказал , что он сказал',\n",
              " 'В настоящее время в качестве « А . В . А .',\n",
              " 'В рамках новой программы « У » в рамках программы работы , а также будет много раз в рамках работы , а также для того , чтобы создать новые и развитие .',\n",
              " 'Откры тые по поводу того , как в каждой группы будет осуществляться в течение каждого совещания , в ходе которого будут проведены в отношении использования , и в рамках подготовки к сведению , что в',\n",
              " 'В результате « А . » ( « О ) и « Г - н . В . The C . 2 . 2 . 2 . 2 . 2 . 2 . 2 .',\n",
              " 'В настоящее время в ходе рассмотрения и содействия осуществлению Декларации о создании Декларации о том , что в области , в 2012 году , в ходе которого было документ , в 2012 году , в',\n",
              " 'В соответствии с тем , что в рамках этой связи с тем , что и в рамках работы проекта , которые были приняты для того , чтобы обеспечить , как и в отношении их деятельности',\n",
              " 'В результате этого было проведено 20 000 человек , в котором были приняты в 19 000 000 человек и учреждений , в котором он был также отметил , что в настоящее время как правило ,',\n",
              " 'В качестве в документе S . 3 . 1 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 3 . 2 . 2 . 2 . 2',\n",
              " 'В течение времени , в котором президент ский совет ский совет , президент ский совет ский совет , сказал , что это не было бы \" не было \" \" и \" О \" О',\n",
              " 'В настоящее время он также в г . г . на Г . В . « О , а также в качестве « А . « О ле ле » ( в г . д',\n",
              " 'Среди различных исследований , которые были также могут использовать и не использование , а также в результате изменения и реинтеграции , и в настоящее время , и для решения проблем , связанных с ними ,',\n",
              " 'В понедельник , что в докладе содержится в силу \" не имеет возможности для оценки условий , и в ходе этого процесса , и в этом случае не было бы , что не менее ,',\n",
              " 'Несмотря на этот момент , в том числе в качестве 40 % - технического сотрудничества в Соединенных Штатах и в области развития , и Канада , которые были приняты меры , которые не менее ,',\n",
              " 'Только несколько раз в отношении этих видов , и большинство из которых является наиболее полез ность , и наиболее важным фактором , которые могут быть более широкого использования в рамках этого процесса , и в',\n",
              " 'В результате по адресу : « Г - н . Г . В . В . « На те же время как и в « О , то же время ( см . в т',\n",
              " 'Каждый фонд в отношении авторов , в котором было проведено в составе « В » в « часов в « За три » в зале заседаний » ( в списке которого находятся в этом году',\n",
              " 'В этом документе можно было также возможность для того , чтобы « При условии , что « создание и возможности , а также для обеспечения и укрепления потенциала в качестве проекта « О , который',\n",
              " 'В . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2 . 2',\n",
              " 'В рамках деятельности , работа , работа по возможности для использования , в целях использования не только для того , чтобы в их более чем не будет легко и не менее , чтобы было бы',\n",
              " '\" Это будет очень трудно , чтобы быть , если он , сказал , что \" О , в котором он был в « The B ill es , который будет использоваться в качестве «',\n",
              " 'Он считает , что каждый день будет способствовать укреплению потенциала и стратегий , которые будут способствовать созданию новых и научно - коммуникационных технологий , которые являются наиболее эффективно .',\n",
              " 'В процесс , возможно , в качестве других исследований , когда он говорит по - прежнему говорит , что он говорит , что он говорит по - английски ): « В .',\n",
              " '« Существует использование людей на их основе , которые будут в их качества , и в связи с их помощью « The F ill i » ( « В . « В этом « Для',\n",
              " 'Все , все равно , что в отношении использования , может быть более эффективно использовать для развития , и его в районе , и на его банк , а также будет в течение нескольких дней',\n",
              " 'Как и другие органы , и другие , в других областях , которые будут также в полной мере , связанные с помощью и впредь доступа к вопросам , которые будут приняты в рамках системы ,',\n",
              " '« Существует не в мире , что он говорит по - английски ): « О , что он говорит по - прежнему не является самым , что он сказал : « О , а также',\n",
              " 'Мы считаем , что это не можем сделать , что мы будем делать , потому что мы считаем , что мы можем , что это будет изменить в рамках « О .',\n",
              " 'У меня есть не будет быть одним из своей усилий по поводу того , чтобы мы не будем в его усилиях , что в его усилиях , а также на то , что в этом',\n",
              " '\" В рамках этого процесса будет более важно и более важным для того , чтобы в области развития и решения проблем , и решения , что это было бы не будет иметь возможность для того',\n",
              " '« В этом мире » всегда будет в первую очередь , так как и другие учреждения , которые вы не можете , чтобы сказать , что в настоящее время « О , и в то']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}