{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте упрощенную версию byte pair encoding (без предварительного разбивания на слова, которое есть в посте о BPE на медиуме –  \n",
    "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10).  \n",
    "\n",
    "Алгоритм должен работать так:  \n",
    "строки с текстом разбиваются на отдельные символы и далее в цикле из N итераций:  \n",
    "    а) считаются статистики встречаемости по парам символов и  \n",
    "    б) топ-K частотных пар склеиваются в один символ  \n",
    "\n",
    "Попробуйте так токенизировать текст с разными параметрами N и K.  \n",
    "Проанализируйте словарь уникальных слов для нескольких наборов параметров - сколько уникальных слов получилось, какой токен самый длинный?  \n",
    "\n",
    "Чтобы получить 1 бонусный балл - зафиксируйте получившийся словарь и токенизируйте с помощью него текст, который ранее не встречался в корпусе (возьмите рандомную новость из яндекс новостей например).  \n",
    "Проанализируйте насколько хорошо токенизировался текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lenta.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = text[:50000].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### считаются статистики встречаемости по парам символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_collect(text):\n",
    "    freqs = {}\n",
    "    for i in range(len(text)-1):\n",
    "        next_symbol = text[i+1]\n",
    "        pair = text[i] + next_symbol\n",
    "        if pair in freqs:\n",
    "            freqs[pair] += 1\n",
    "        else:\n",
    "            freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### топ-K частотных пар склеиваются в один символ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger(text, dictionary, k):\n",
    "    sort_dict = {}\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = sorted(keys, key=lambda x: dictionary[x], reverse=True)\n",
    "    keys = keys[:k]\n",
    "    for key in keys:\n",
    "        sort_dict[key] = dictionary[key]\n",
    "    result = ()\n",
    "    process = True\n",
    "    for ind, sym in enumerate(text):\n",
    "        if process:\n",
    "            if ind < len(text)-1:\n",
    "                next_symbol = text[ind+1]\n",
    "            else:\n",
    "                result += tuple(sym, )\n",
    "                break\n",
    "            pair = sym + next_symbol\n",
    "            if pair in sort_dict: \n",
    "                result += (pair, )\n",
    "                process = False\n",
    "            else:\n",
    "                result += (sym, )\n",
    "        else:\n",
    "            process = True\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# вычисляем bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________\n",
      "n = 3 ,k = 5\n",
      "n of tokens: 42881\n",
      "б/о/и /у/ с/о/п/о/ц/к/и/на/ /и /д/р/у/с/к/ен/и/к/ /з/а/к/о/н/ч/и/л/и/с/ь/ о/т/ст/у/п/л/ен/и/е/м/ /г/е/р/м/а/н/ц/е/в/./ /н/е/п/р/и/я/т/е/л/ь/,/ п/р/и/б/л/и/з/и/в/ш/и/с/ь/ с/ с/е/в/е/ра/ /к/ о/с/ов/ц/у/ /на/ч/а/л/ /а\n",
      "Longest token: и \n",
      "_______________________________\n",
      "n = 5 ,k = 12\n",
      "n of tokens: 35121\n",
      "б/о/и /у/ с/о/п/о/ц/к/ин/а /и /д/р/у/ск/ени/к/ /за/ко/н/ч/и/ли/с/ь/ о/т/ст/у/п/л/ени/е/м /г/ер/м/ан/ц/е/в/./ н/е/п/ри/я/т/ел/ь/, /п/ри/б/ли/з/и/в/ш/и/с/ь/ с/ с/е/в/ер/а /к/ о/с/ов/ц/у/ н/а/ч/ал/ /ар/т/ил/л/ер/и/й/ск/у/ю/ б/ор/ь/б/у/ с/ /к\n",
      "Longest token: ени\n",
      "_______________________________\n",
      "n = 10 ,k = 25\n",
      "n of tokens: 26758\n",
      "бо/и /у/ со/по/ц/ки/на /и /д/ру/ск/ени/к /за/кон/ч/ил/ис/ь/ от/сту/п/лени/е/м /г/ер/м/ан/ц/е/в/./ н/е/при/я/тель/, /при/б/ли/зи/вши/с/ь/ с/ се/в/ера/ /к/ о/сов/ц/у/ на/ч/ал/ /ар/т/ил/л/ер/и/й/ск/у/ю/ б/ор/ь/бу/ с/ /к/ре/по/ст/ь/ю/./ в /ар/т/ил/л/ер/и/й/ско/м /бо/ю/ при/ни/ма/ют\n",
      "Longest token: ности \n",
      "_______________________________\n",
      "n = 20 ,k = 50\n",
      "n of tokens: 19305\n",
      "бо/и /у/ со/по/ц/ки/на/ и/ д/ру/ск/ени/к/ за/кон/чи/лись/ от/ступ/лени/ем/ г/ер/ман/це/в/. /не/при/ятель/, /при/бли/зи/вши/сь/ с/ се/вер/а /к/ о/сов/цу/ на/ча/л /артиллери/йск/ую/ б/ор/ь/бу/ с/ к/ре/по/ст/ью/. в /артиллери/й/ском/ бо/ю/ при/ни/ма/ют/ участ/и/е /т/я/ж/ел/ы/е /ка/ли/б/ры/. с/ ран/не/го/ у/тра/ 1/4/ сентября /ого/нь/ д/ос/ти/г/ з\n",
      "Longest token: рачаево-черкеси\n",
      "_______________________________\n",
      "n = 30 ,k = 60\n",
      "n of tokens: 16395\n",
      "бо/и у/ со/по/ц/ки/на и/ дру/ск/ени/к/ закон/ч/или/сь/ от/ступлени/ем /г/ерман/цев/. не/приятель/, при/бли/зи/вшись/ с/ с/е/вер/а /к/ о/сов/цу/ начал/ артиллери/йск/ую/ бор/ь/бу/ с к/ре/пост/ью/. в /артиллери/йском/ бо/ю/ принимают/ участ/ие /тяжел/ы/е /кали/б/ры/. с/ ран/него/ у/тра/ 1/4/ сентября /огонь/ дости/г/ зна/читель/ного/ на/пря/жени/я. по/пыт/ка/ гер/ман/ской /пе/хоты/ проб/ить/ся /бли/же /к/ кре/по/сти /от/ра/жен/а/. в \n",
      "Longest token:  сентября 1914 года\n"
     ]
    }
   ],
   "source": [
    "def bpe(text, n, k):\n",
    "    for i in range(n):\n",
    "        frequencies = stats_collect(text)\n",
    "        text = merger(text, frequencies, k)\n",
    "    return text\n",
    "\n",
    "n = [3, 5, 10, 20, 30]\n",
    "k = [5, 12, 25, 50, 60]\n",
    "\n",
    "\n",
    "\n",
    "for i,j in zip(n, k):\n",
    "    result = bpe(text, i, j)\n",
    "    print('_______________________________')\n",
    "    print('n = ' + str(i), ',k = ' + str(j))\n",
    "    print('n of tokens: ' + str(len(result)))\n",
    "    print('/'.join(result[:100]))\n",
    "    print('Longest token: ' + str(sorted(result, key=lambda x: len(x), reverse=True)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По мере увеличения параметров мы видим, как буквы складываются в \"морфемы\", \"морфемы\" в \"фрагменты слов\" и \"слова\", \"слова\" в \"словосочетания\".\n",
    "Естественно, что это все происходит во многом случайно (в зависимости от обучающего корпуса), потому что мы никак не оптимизируем процесс отбора слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Обучите токенизатор из tokenizers на текстовом корпусе. \n",
    "Рассчитайте статистики для idf по корпусу, используя обученный словарь (разбейте корпус на \"документы\" по новым \n",
    "строкам, каждый \"документ\" токенизируйте, для каждого слова посчитайте, в скольких документах оно встречается и рассчитайте \n",
    "idf разделив общее количество документов на это число, возьмите логарифм от полученного числа). \n",
    "Векторизуйте текст (в мешок слов) аналогично TfidfVectorizer, используя токенизатор и idf статистики \n",
    "(инициализируйте*** пустую матрицу размером (N документов, K слов в словаре) и в цикле по всем документам постепенно \n",
    "заполните ее - токенизируйте документ, рассчитайте TF каждого слова (количество вхождений в документе поделить на общее \n",
    "количество слов в документе), умножьте TF на IDF и, используя индексы слов в словаре, запишите получившееся значение в матрицу)\n",
    "\n",
    "Формулу для TFIDF можете уточнить тут - https://ru.wikipedia.org/wiki/TF-IDF\n",
    "\n",
    "***Чтобы инициализировать разреженную матрицу используйте scipy:\n",
    "from scipy.sparse import lil_matrix\n",
    "X = lil_matrix(N, K)\n",
    "Обучите классификатор на полученных векторах и оцените на кросс-валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "from collections import Counter, defaultdict\n",
    "from tokenizers import CharBPETokenizer, Tokenizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92620247, 0.92403195, 0.92663657, 0.92342421, 0.92871407])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_ok.csv')\n",
    "data = df['text'].tolist()\n",
    "df['text'].to_csv('data/data.txt', index=False)\n",
    "subtoken = CharBPETokenizer()\n",
    "subtoken.train('data/data.txt', vocab_size=3000)\n",
    "subtoken.save('subtoken')\n",
    "subtoken = Tokenizer.from_file('subtoken')\n",
    "\n",
    "N = len(data)\n",
    "K = len(subtoken.get_vocab())\n",
    "\n",
    "X = lil_matrix((N, K))\n",
    "X_idf = lil_matrix((N, K))\n",
    "\n",
    "for i, text in enumerate(data):\n",
    "    token_ids = subtoken.encode(text).ids\n",
    "    for t in token_ids:\n",
    "        X[i, t] += 1\n",
    "        if X_idf[i, t] == 0:\n",
    "            X_idf[i, t] = 1\n",
    "\n",
    "idf = pd.Series(X_idf.sum(axis=0).tolist()[0])\n",
    "idf = idf.apply(lambda x: np.log((1 + idf.shape[0]) / (1 + x)) + 1)\n",
    "\n",
    "X = X.multiply(lil_matrix(idf.tolist()))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, stratify=df['label'])\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", max_iter=30)\n",
    "cross_val_score(clf, X_train, y_train, scoring=\"f1_micro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
