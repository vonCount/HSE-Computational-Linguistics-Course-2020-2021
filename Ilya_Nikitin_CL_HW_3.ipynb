{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDu9DkxLevQY"
   },
   "source": [
    "# Задание 1. Реализуйте алгоритм Symspell\n",
    "\n",
    "Он похож на алгоритм Норвига, но проще и быстрее. Там к словам в словаре применяется только одна операция - удаление символа. Чтобы найти исправление из слова тоже удаляются символы и сравниваются с теми, что хранятся в словаре. Оцените качество полученного алгоритма теми же тремя метриками.\n",
    "\n",
    "https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qEvvfi0zOJv",
    "outputId": "7862f6f8-1337-455c-ad65-c789d82d62f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in /opt/anaconda3/lib/python3.8/site-packages (0.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "\n",
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pL4dfjmve4CG"
   },
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))\n",
    "\n",
    "\n",
    "corpus = open('wiki_data.txt', encoding='utf8').read()\n",
    "\n",
    "WORDS = Counter(re.findall('\\w+', corpus.lower()))\n",
    "\n",
    "# фунцкия расчета вероятности слова\n",
    "N = sum(WORDS.values())\n",
    "def P(word, N=N): \n",
    "    \"Вычисляем вероятность слова\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "corpus = set(re.findall('\\w+', corpus.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wDhHTNBRfjBO"
   },
   "outputs": [],
   "source": [
    "def collect_deleted(dictionary, word, depth=1):\n",
    "    if len(word) > depth:\n",
    "        if depth == 1:\n",
    "                for i in range(len(word)):\n",
    "                    current = word[:i] + word[(i+1):]\n",
    "                    if current not in dictionary:\n",
    "                        dictionary[current] = [word]\n",
    "                    else:\n",
    "                        dictionary[current].append(word)\n",
    "        if depth == 2:\n",
    "            for i in range(len(word)):\n",
    "                for k in range(i+1, len(word)):\n",
    "                    current = word[:i] + word[(i+1):k] + word[k+1:]\n",
    "                    if current not in dictionary:\n",
    "                        dictionary[current] = [word]\n",
    "                    else:\n",
    "                        dictionary[current].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0nnrVrqvfkjM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 1.58 s, total: 31.4 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocab_correct = {}\n",
    "vocab_1_letter_deleted = {}\n",
    "vocab_2_letters_deleted = {}\n",
    "\n",
    "for word in corpus:\n",
    "    vocab_correct[word] = [word]\n",
    "    collect_deleted(vocab_1_letter_deleted, word, depth=1)\n",
    "    collect_deleted(vocab_2_letters_deleted, word, depth=2)\n",
    "    \n",
    "vocabs = [vocab_correct, vocab_1_letter_deleted, vocab_2_letters_deleted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_candidate(c_candidates):\n",
    "    if len(c_candidates) == 0:\n",
    "        return None\n",
    "    chosen_candidate = max(c_candidates, key=P) \n",
    "    return chosen_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocabs(word, vocabs, candidates, start_weight):\n",
    "    for i in range(len(vocabs)): \n",
    "        vocab = vocabs[i]\n",
    "        if word in vocab:\n",
    "            weight = max(i, start_weight)\n",
    "            candidates[weight] += vocab[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SLNHiTPCfzHh"
   },
   "outputs": [],
   "source": [
    "def correction(word):\n",
    "    correct = word\n",
    "    candidates = [[], [], [], [], []] \n",
    "    check_vocabs(word, vocabs, candidates, 0)\n",
    "    for i in range(len(word)):\n",
    "        current_1 = word[:i] + word[(i+1):]\n",
    "        check_vocabs(current_1, vocabs, candidates, 1)\n",
    "        for k in range(i+1, len(word)):\n",
    "            current_2 = word[:i] + word[(i+1):k] + word[k+1:]\n",
    "            check_vocabs(current_2, vocabs, candidates, 2)\n",
    "    for c in candidates:\n",
    "        if len(c) > 0:\n",
    "            return choose_candidate(c)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'лондон'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#проверим работу системы\n",
    "\n",
    "correction('ондон')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "спасла\n",
      "карлуш\n",
      "с\n",
      "калатафими\n",
      "по\n",
      "науке\n",
      "и\n",
      "выразил\n",
      "путёвку\n",
      "вошли\n",
      "карлуша\n",
      "калушаточки\n",
      "утковка\n"
     ]
    }
   ],
   "source": [
    "text_fun = ['сяпала', 'калуша', 'с', 'калушатами', 'по', 'напушке', 'и', 'увазила', 'бутявку', 'волит', 'калушата', 'калушаточки', 'бутявка'\n",
    "]\n",
    "\n",
    "for i in text_fun:\n",
    "    print(correction(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J6bLyfRAf4aC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "#считаем метрики\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        pred = correction(pair[1])\n",
    "\n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LNPphWoaikG5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct/total: 0.8579420579420579\n",
      "mistaken_fixed/total_mistaken:  0.4405218726016884\n",
      "correct_broken/total_correct:  0.07959113357069025\n"
     ]
    }
   ],
   "source": [
    "print('correct/total:', correct/total)\n",
    "print('mistaken_fixed/total_mistaken: ', mistaken_fixed/total_mistaken)\n",
    "print('correct_broken/total_correct: ', correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dSCF4EsfNws"
   },
   "source": [
    "\n",
    "# Задание 2. Добавьте к полученному алгоритму исправления (Symspell) триграммную модель и проверьте, улучшает ли она качество. \n",
    "Триграммную модель нужно вставить туда, где у вас выбирается один из нескольких кандидатов на исправление.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Yh1pxalJfEsE"
   },
   "outputs": [],
   "source": [
    "corpus = open('wiki_data.txt', encoding='utf8').read()\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text)\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "corpus_wiki = [['<start>'] + sent + ['<end>'] for sent in preprocess(corpus)]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8Fbpr_iMis07"
   },
   "outputs": [],
   "source": [
    "trigrams = {}\n",
    "\n",
    "for sent in corpus_wiki:\n",
    "    for i in range(1 , len(sent)-1):\n",
    "        token = sent[i]\n",
    "        context = (sent[i-1], sent[i+1])\n",
    "        if context not in trigrams:\n",
    "            trigrams[context] = {}\n",
    "            trigrams[context][token] = 1\n",
    "        else:\n",
    "            if token not in trigrams[context]:\n",
    "                trigrams[context][token] = 1\n",
    "            else:\n",
    "                trigrams[context][token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = ['<start>'] + [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)] + ['<end>']\n",
    "    tokens_2 = ['<start>'] + [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)] + ['<end>']\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем вероятность слова в контексте\n",
    "def Prob_in_context(word, context):\n",
    "    if context not in trigrams:\n",
    "        return 1\n",
    "    else:\n",
    "        counter = 0\n",
    "        trigram = trigrams[context]        \n",
    "        for w in trigram:\n",
    "            counter += trigram[w]\n",
    "        if word in trigram:\n",
    "            return trigram[word]/counter\n",
    "        else:\n",
    "            return 1/(100*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FcCENcy0i_N4"
   },
   "outputs": [],
   "source": [
    "def choose_candidate(c_candidates, context):\n",
    "    if len(c_candidates) == 0:\n",
    "        return None\n",
    "    chosen_candidate = max(c_candidates, key=lambda x: P(x) * Prob_in_context(x, context)) \n",
    "    return chosen_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6SmwYrWQjC0t"
   },
   "outputs": [],
   "source": [
    "#добавим контекст в функцию исправления\n",
    "def correction(word, context):\n",
    "    correct = word\n",
    "    candidates = [[], [], [], [], []] \n",
    "    check_vocabs(word, vocabs, candidates, 0)\n",
    "    for i in range(len(word)):\n",
    "        current_1 = word[:i] + word[(i+1):]\n",
    "        check_vocabs(current_1, vocabs, candidates, 1)\n",
    "        for k in range(i+1, len(word)):\n",
    "            current_2 = word[:i] + word[(i+1):k] + word[k+1:]\n",
    "            check_vocabs(current_2, vocabs, candidates, 2)\n",
    "    for c in candidates:\n",
    "        if len(c) > 0:\n",
    "            return choose_candidate(c, context)\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kf9iV9XxjFg2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# считаем метрики\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for k in range(1, len(word_pairs)-1):\n",
    "        pair = word_pairs[k]\n",
    "        previous = word_pairs[k-1]\n",
    "        following = word_pairs[k+1]\n",
    "        context = (previous[1], following[1])\n",
    "        pred = correction(pair[1], context)\n",
    "        \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1\n",
    "\n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nklgmv1rjKyp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct/total:  0.859040959040959\n",
      "mistaken_fixed/total_mistaken:  0.44896392939370683\n",
      "correct_broken/total_correct:  0.07959113357069025\n"
     ]
    }
   ],
   "source": [
    "print('correct/total: ', correct/total)\n",
    "print('mistaken_fixed/total_mistaken: ', mistaken_fixed/total_mistaken)\n",
    "print('correct_broken/total_correct: ', correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сравнению с предыдущим результатом:\n",
    "\n",
    "correct/total: 0.8579420579420579  \n",
    "mistaken_fixed/total_mistaken 0.4405218726016884  \n",
    "correct_broken/total_correct:  0.07959113357069025  \n",
    "\n",
    "...качество выросло по первым двум метрикам, но незначительно."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ilya's CL HW 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
