{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ilya's CL HW 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0RA-PEnJQLc"
      },
      "source": [
        "Скачайте вот этот текст -  https://github.com/mannefedov/compling_nlp_hse_course/blob/master/data/zhivago.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Fz3D5rIprC"
      },
      "source": [
        "### Задание 1 (2 балла)\n",
        "Отчистите текст от мусора (тэгов, хешей и тп) с помощью регулярных выражений. Постарайтесь убрать весь мусор, но если что-то удалить не получается, то не мучайтесь. Главное, чтобы мусор не проявлялся в результатах следующих заданий."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4vSw3T3msMM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "b50b7cb5-a568-48b8-80b6-fa9ca76439c7"
      },
      "source": [
        "#Скачайте вот этот текст - https://github.com/mannefedov/compling_nlp_hse_course/blob/master/data/zhivago.txt\n",
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/zhivago.txt'\n",
        "text = requests.get(url).text\n",
        "text[:1000]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<author><first-name>Борис</first-name><middle-name>Леонидович</middle-name><last-name>Пастернак</last-name></author>\\n<book-title>Доктор Живаго</book-title>\\n<annotation><p>«Доктор Живаго» - итоговое произведение Бориса Пастернака, книга всей его жизни. Этот роман принес его автору мировую известность и Нобелевскую премию, присуждение которой обернулось для поэта оголтелой политической травлей, обвинениями в «измене Родине» и в результате стоило ему жизни.</p>\\n<p>«Доктор Живаго» - роман, сама ткань которого убедительнее свидетельствует о чуде, чем все размышления доктора и обобщения автора. Человек, который так пишет, бесконечно много пережил и передумал, и главные его чувства на свете  - восхищенное умиление и слезное сострадание; конечно, есть в его мире место и презрению, и холодному отстранению  - но не в них суть. Роман Пастернака  - оплакивание прежних заблуждений и их жертв; те, кто не разделяет молитвенного восторга перед миром, достойны прежде всего жалости. Перечитывать «Доктор'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "eqIWoYaspa83",
        "outputId": "7a1941a2-5eee-4146-e22b-caf7019db49a"
      },
      "source": [
        "#Отчистите текст от мусора (тэгов, хешей и тп) с помощью регулярных выражений.\n",
        "import re\n",
        "text = re.sub(u'\\xa0', ' ', text)\n",
        "text = re.sub(r'<[^>]+>', ' ', text)\n",
        "text = re.sub(r'\\n', ' ', text)\n",
        "text = re.sub('  +', ' ', text)\n",
        "text[:1000]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Борис Леонидович Пастернак Доктор Живаго «Доктор Живаго» - итоговое произведение Бориса Пастернака, книга всей его жизни. Этот роман принес его автору мировую известность и Нобелевскую премию, присуждение которой обернулось для поэта оголтелой политической травлей, обвинениями в «измене Родине» и в результате стоило ему жизни. «Доктор Живаго» - роман, сама ткань которого убедительнее свидетельствует о чуде, чем все размышления доктора и обобщения автора. Человек, который так пишет, бесконечно много пережил и передумал, и главные его чувства на свете - восхищенное умиление и слезное сострадание; конечно, есть в его мире место и презрению, и холодному отстранению - но не в них суть. Роман Пастернака - оплакивание прежних заблуждений и их жертв; те, кто не разделяет молитвенного восторга перед миром, достойны прежде всего жалости. Перечитывать «Доктора Живаго» стоит именно тогда, когда кажется, что жить не стоит. Тогда десять строк из этого романа могут сделать то же, что делает любовь в'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwKdnFkFIrz-"
      },
      "source": [
        "### Задание 2. (2 балла) \n",
        "Приведите очищенный текст к нижнему регистру, удалите все знаки пунктуации, разделите на предложения библиотекой rusenttokenize, токенизируйте библиотекой razdel_tokenize. \n",
        "Ответьте на следующие вопросы:\n",
        "1) есть ли в тексте повторяющиеся корректные предложения? если да то какие? (если находится мусор то вернитесь к заданию 1 и постарайтесь избавиться от него)\n",
        "2) какой самый частотный токен в тексте длиннее 6 символов?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUhi1leBJqIs",
        "outputId": "1356c902-cbc8-40fa-e135-ddaf946afc1d"
      },
      "source": [
        "!pip install rusenttokenize\n",
        "!pip install razdel\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "\n",
        "#Приведите очищенный текст к нижнему регистру\n",
        "text = text.lower()\n",
        "\n",
        "#Удалите все знаки пунктуации, токенизируйте библиотекой razdel_tokenize\n",
        "punctuation = f'{punctuation}«»–...'\n",
        "tokens = [token.text.strip(punctuation) for token in razdel_tokenize(text) \n",
        "          if token.text not in punctuation]\n",
        "tokens = [token for token in tokens if token]\n",
        "print(tokens[:15])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rusenttokenize\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Installing collected packages: rusenttokenize\n",
            "Successfully installed rusenttokenize-0.0.5\n",
            "Collecting razdel\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n",
            "['борис', 'леонидович', 'пастернак', 'доктор', 'живаго', 'доктор', 'живаго', 'итоговое', 'произведение', 'бориса', 'пастернака', 'книга', 'всей', 'его', 'жизни']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILz0WcWdJzxP",
        "outputId": "2d6fea53-c545-4343-de65-22e39ca8a349"
      },
      "source": [
        "#Разделите на предложения библиотекой rusenttokenize\n",
        "sentences = ru_sent_tokenize(text)\n",
        "\n",
        "#Есть ли в тексте повторяющиеся корректные предложения? Если да, то какие?\n",
        "def sorted_dict(list):\n",
        "    counter = dict(Counter(list)).items()\n",
        "    result = sorted(counter, key=lambda x: x[1], reverse=True)\n",
        "    return result\n",
        "\n",
        "[sent for sent in sorted_dict(sentences) if sent[1] > 1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('свеча горела на столе, свеча горела.', 3),\n",
              " ('парило.', 2),\n",
              " ('странно.', 2),\n",
              " ('толпа росла.', 2),\n",
              " ('да.', 2),\n",
              " ('он открыл глаза.', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv-AtzHhd3PR",
        "outputId": "5606c4e7-2b50-482f-d0f2-5b6fec07983c"
      },
      "source": [
        "[word for word in sorted_dict(tokens) if len(word[0]) > 6][0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('андреевич', 289)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVz8OVDCIuER"
      },
      "source": [
        "### Задание 3. (2 балла)\n",
        "\n",
        "Сделайте стемминг и найдите по несколько частотных примеров на каждый из видов ошибок:\n",
        "1) два разных слова ошибочно свелись к одинаковой основе\n",
        "3) слово не изменилось после стемминга (слово должно быть русским и длиннее 4 символов)\n",
        "\n",
        "***не делайте это задание полностью вручную, напишите правила, которые отловят потенциальные ошибки и выберите из них***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rcF2vHVmtoX"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('russian')\n",
        "words = [token.text for token in list(razdel_tokenize(text))]\n",
        "\n",
        "stemms = {}\n",
        "st_count = Counter(words)\n",
        "\n",
        "for word in words:\n",
        "    stemms[word] = (st_count[stemmer.stem(word)],stemmer.stem(word))\n",
        "\n",
        "stemms2words = {} \n",
        "for key, value in stemms.items(): \n",
        "    if value not in stemms2words: \n",
        "        stemms2words[value] = [key]\n",
        "    else:\n",
        "        stemms2words[value].append(key) \n",
        "        \n",
        "sorted_stemms2words = sorted(stemms2words.items(), reverse = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0C3p-APqSxk"
      },
      "source": [
        "Слова, которые ошибочно свелись к одной основе:\n",
        "\n",
        "((2933, 'не'), ['не', 'ней', 'нее', 'нею'])  \n",
        "((1577, 'он'), ['они', 'он', 'оно', 'она', 'оной'])  \n",
        "((1288, 'как'), ['как', 'какие', 'какой', 'какая', 'каком', 'каким', 'какую', 'какое', 'каких', 'какою', 'какому', 'каков', 'какими', 'какого', 'кака', 'каки'])  \n",
        "((990, 'а'), ['а', 'ай'])  \n",
        "((964, 'по'), ['по', 'поила', 'поили'])  \n",
        "((761, 'все'), ['всей', 'все', 'всею', 'всё'])  \n",
        "((729, 'у'), ['у', 'уа'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3OIsbd-FpVF"
      },
      "source": [
        "Слова, которые не изменились после стемминга:\n",
        "\n",
        "s = {}\n",
        "for word in words:\n",
        "    s[st_count[word],word] = stemmer.stem(word)\n",
        "keyEQvalue = {}\n",
        "for key, value in s.items():\n",
        "    if key[1] == value and len(key[1])>4:\n",
        "        keyEQvalue[key] = value\n",
        "\n",
        "sorted_keyEQvalue = sorted(keyEQvalue.items(), reverse = True)\n",
        "#print(sorted_keyEQvalue)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri3ok2dHxr9v",
        "outputId": "8c91e5d2-23d8-4331-b486-7f999c0c7db8"
      },
      "source": [
        "sorted_keyEQvalue[:10]"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((289, 'андреевич'), 'андреевич'),\n",
              " ((226, 'доктор'), 'доктор'),\n",
              " ((172, 'перед'), 'перед'),\n",
              " ((162, 'может'), 'может'),\n",
              " ((144, 'через'), 'через'),\n",
              " ((141, 'будет'), 'будет'),\n",
              " ((127, 'сейчас'), 'сейчас'),\n",
              " ((112, 'человек'), 'человек'),\n",
              " ((109, 'вдруг'), 'вдруг'),\n",
              " ((65, 'поезд'), 'поезд')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmRFJSRVyA05"
      },
      "source": [
        "Слова, которые не изменились после стемминга:\n",
        "\n",
        "((289, 'андреевич'), 'андреевич')  \n",
        "((226, 'доктор'), 'доктор')  \n",
        "((172, 'перед'), 'перед')  \n",
        "((162, 'может'), 'может')  \n",
        "((144, 'через'), 'через')  \n",
        "((141, 'будет'), 'будет')  \n",
        "((127, 'сейчас'), 'сейчас')  \n",
        "((112, 'человек'), 'человек')  \n",
        "((109, 'вдруг'), 'вдруг')  \n",
        "((65, 'поезд'), 'поезд')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L4iskiOIxRk"
      },
      "source": [
        "### Задание 4 (1 балл)\n",
        "Проанализируйте список стоп-слов из нлтк (для русского). Какие ещё слова вы бы туда добавили? (5 слов будет достаточно, не забудьте аргументировать ваш выбор)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBcaUuOJmuVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af4ca71-b559-4c0c-a5a9-b430887ac093"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stops = stopwords.words('russian')\n",
        "\n",
        "ruscorpora = ['и', 'в', 'не', 'на', 'я', 'быть', 'он', 'с', 'что', 'а', 'по', 'это', 'она', 'этот', 'к', 'но', 'они', 'мы', 'как', 'из', 'у', 'который', 'то', 'за', 'свой', 'что', 'весь', 'год', 'от', 'так', 'о', 'для', 'ты', 'же', 'все', 'тот', 'мочь', 'вы', 'человек', 'такой', 'его', 'сказать', 'только', 'или', 'ещё', 'бы', 'себя', 'один', 'как', 'уже', 'до', 'время', 'если', 'сам', 'когда', 'другой', 'вот', 'говорить', 'наш', 'мой', 'знать', 'стать', 'при', 'чтобы', 'дело', 'жизнь', 'кто', 'первый', 'очень', 'два', 'день', 'её', 'новый', 'рука', 'даже', 'во', 'со', 'раз', 'где', 'там', 'под', 'можно', 'ну', 'какой', 'после', 'их', 'работа', 'без', 'самый', 'потом', 'надо', 'хотеть', 'ли', 'слово', 'идти', 'большой', 'должен', 'место', 'иметь', 'ничто']\n",
        "\n",
        "for word in ruscorpora:\n",
        "    if word.replace('ё', 'е') not in stops:\n",
        "        print(word)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "это\n",
            "который\n",
            "свой\n",
            "весь\n",
            "год\n",
            "мочь\n",
            "человек\n",
            "сказать\n",
            "время\n",
            "говорить\n",
            "наш\n",
            "знать\n",
            "стать\n",
            "дело\n",
            "жизнь\n",
            "первый\n",
            "очень\n",
            "день\n",
            "новый\n",
            "рука\n",
            "работа\n",
            "самый\n",
            "хотеть\n",
            "слово\n",
            "идти\n",
            "большой\n",
            "должен\n",
            "место\n",
            "иметь\n",
            "ничто\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHxQMkP9U59i"
      },
      "source": [
        "Ответ: все служебные слова из списка 1000 самых встречаемых согласно НКРЯ, которые еще не вошли в NLTK stopwords - это, который, свой, весь, наш, очень."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eczDO-XPI0AV"
      },
      "source": [
        "### Задание 5 (3 балла)\n",
        "\n",
        "Предобработайте текст двумя способами:\n",
        "1) лемматизируйте токены с помощью pymorphy2\n",
        "2) лемматизируйте текст с помощью mystem3 \n",
        "\n",
        "Ответьте на вопрос:\n",
        "Что в данном случае лучше для лемматизации mystem или pymorphy?\n",
        "\n",
        "Важно, чтобы ответы на вопросы были аргументированы (как минимум три аргумента).  Анализируйте результаты с языковой точки зрения. Скорость работы простота интерфейса не являются преимуществами или недостатками в рамках этого задания. Идеальный аргумент - библиотека x неправильно обрабатывает вот такое слово/класс слов. \n",
        "\n",
        "\n",
        "Оформите задания в jupyter тетрадке, сохраните её на гитхаб (или другой ресурс) и вставьте ссылку в поле ниже.\n",
        "Перед отправкой проверьте, что ваша тетрадка отображается правильно (гитхаб, например, не умеет сворачивать длинные выводы, от чего тетрадка растягивается и становится нечитаемой)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "PIqxOXNANRGP",
        "outputId": "d290ef8d-9127-4ee8-9b11-edb98b026ef6"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install pymorphy2\n",
        "!pip install pymystem3\n",
        "\n",
        "import pymorphy2\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "mystem = Mystem()\n",
        "morph =  pymorphy2.MorphAnalyzer()\n",
        "\n",
        "lemmas_pm2 = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "lemmas_mt3 = mystem.analyze(text)\n",
        "\n",
        "lemmas_mt3_clean = []\n",
        "for i in lemmas_mt3:\n",
        "  if re.findall(r'\\w+', i) != []:\n",
        "    lemmas_mt3_clean.append(i)\n",
        "\n",
        "for i in range(150):\n",
        "  if lemmas_pm2[i] != lemmas_mt3_clean[i]:\n",
        "    print(i, all_tokens[i], lemmas_pm2[i], lemmas_mt3_clean[i])\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pymystem3) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3) (3.0.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-29e209d9f152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlemmas_pm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlemmas_mt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlemmas_mt3_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHuCDk4XyJCY"
      },
      "source": [
        "lemmas_comparison = pd.DataFrame({'Токен': unique_tokens,\n",
        "                                  'Лемма pymorphy2': pymorphy2_lemmas,\n",
        "                                  'Лемма mystem': mystem_lemmas})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}