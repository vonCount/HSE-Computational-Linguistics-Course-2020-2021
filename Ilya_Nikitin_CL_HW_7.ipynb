{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Ilya Nikitin CL HW 7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLrWtcHse0WV"
      },
      "source": [
        "# Задание 1 (3 балла)  \n",
        "Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно, и с помощью модели, взятой с rusvectores (например вот этой - http://vectors.nlpl.eu/repository/20/180.zip). Обучите 2 модели по определению перефразирования на получившихся векторах и проверьте, что работает лучше. \n",
        "Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие.  \n",
        "!!!! ВАЖНО: Оценивать модели нужно с помощью кросс-валидации (в семинаре не кросс-валидация)! Метрика - f1.\n"
      ],
      "id": "ZLrWtcHse0WV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuDetRe0RaNW"
      },
      "source": [
        "## Загражаем библиотеки и данные"
      ],
      "id": "xuDetRe0RaNW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sized-electricity",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cabb8a-4046-4b40-a4ab-e85c3a95e0ce"
      },
      "source": [
        "!pip install pymorphy2\n",
        "!pip install razdel\n",
        "\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stops = set(nltk.corpus.stopwords.words('russian'))\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "\n",
        "from string import punctuation\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "\n",
        "\n",
        "def normalize(text):\n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = [token.text for token in list(razdel_tokenize(text))]\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/vonCount/HSE-Computational-Linguistics-Course-2020-2021/main/paraphrases.xml\n",
        "!wget https://raw.githubusercontent.com/vonCount/HSE-Computational-Linguistics-Course-2020-2021/main/corpus.xml\n",
        "\n",
        "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})\n",
        "\n",
        "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
        "data['text_2_norm'] = data['text_2'].apply(normalize)"
      ],
      "id": "sized-electricity",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: razdel in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "--2021-06-28 18:19:44--  https://raw.githubusercontent.com/vonCount/HSE-Computational-Linguistics-Course-2020-2021/main/paraphrases.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3701830 (3.5M) [text/plain]\n",
            "Saving to: ‘paraphrases.xml.1’\n",
            "\n",
            "paraphrases.xml.1   100%[===================>]   3.53M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-06-28 18:19:44 (77.0 MB/s) - ‘paraphrases.xml.1’ saved [3701830/3701830]\n",
            "\n",
            "--2021-06-28 18:19:44--  https://raw.githubusercontent.com/vonCount/HSE-Computational-Linguistics-Course-2020-2021/main/corpus.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4969085 (4.7M) [text/plain]\n",
            "Saving to: ‘corpus.xml.1’\n",
            "\n",
            "corpus.xml.1        100%[===================>]   4.74M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-06-28 18:19:45 (61.7 MB/s) - ‘corpus.xml.1’ saved [4969085/4969085]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "english-writing",
        "outputId": "53ecdf11-07cb-456c-8b49-24915317c608"
      },
      "source": [
        "#Смотрим на данные \n",
        "data.head()"
      ],
      "id": "english-writing",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>0</td>\n",
              "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
              "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>0</td>\n",
              "      <td>право полицейский проникновение жилище решить ...</td>\n",
              "      <td>правило внесудебный проникновение полицейский ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
              "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
              "      <td>0</td>\n",
              "      <td>президент египет ввести чрезвычайный положение...</td>\n",
              "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>-1</td>\n",
              "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
              "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
              "      <td>0</td>\n",
              "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
              "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_1  ...                                        text_2_norm\n",
              "0  Полицейским разрешат стрелять на поражение по ...  ...  полиция мочь разрешить стрелять хулиган травма...\n",
              "1  Право полицейских на проникновение в жилище ре...  ...  правило внесудебный проникновение полицейский ...\n",
              "2  Президент Египта ввел чрезвычайное положение в...  ...  власть египет угрожать ввести страна чрезвычай...\n",
              "3  Вернувшихся из Сирии россиян волнует вопрос тр...  ...      самолёт мчс вывезти россиянин разрушить сирия\n",
              "4  В Москву из Сирии вернулись 2 самолета МЧС с р...  ...      самолёт мчс вывезти россиянин разрушить сирия\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "received-sandwich",
        "outputId": "c77890d9-df02-4a09-b4ae-751480805e44"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/lenta.txt.zip -O lenta.txt.zip\n",
        "!unzip lenta.txt.zip\n",
        "!wget http://vectors.nlpl.eu/repository/20/180.zip\n",
        "!unzip 180.zip\n",
        "import gensim\n",
        "\n",
        "lenta = open('lenta.txt').read().splitlines()[:5000]\n",
        "data_lenta = [normalize(text) for text in lenta]\n",
        "\n",
        "my_w2v = gensim.models.Word2Vec([text.split() for text in data_lenta], size=50, sg=1)\n",
        "\n",
        "ready_w2v = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "id": "received-sandwich",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-28 18:20:11--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/lenta.txt.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5723675 (5.5M) [application/zip]\n",
            "Saving to: ‘lenta.txt.zip’\n",
            "\n",
            "lenta.txt.zip       100%[===================>]   5.46M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-06-28 18:20:11 (72.6 MB/s) - ‘lenta.txt.zip’ saved [5723675/5723675]\n",
            "\n",
            "Archive:  lenta.txt.zip\n",
            "replace lenta.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace __MACOSX/._lenta.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "--2021-06-28 18:20:32--  http://vectors.nlpl.eu/repository/20/180.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 484452317 (462M) [application/zip]\n",
            "Saving to: ‘180.zip.2’\n",
            "\n",
            "180.zip.2           100%[===================>] 462.01M  25.2MB/s    in 53s     \n",
            "\n",
            "2021-06-28 18:21:25 (8.76 MB/s) - ‘180.zip.2’ saved [484452317/484452317]\n",
            "\n",
            "Archive:  180.zip\n",
            "replace meta.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace model.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace model.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace README? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "southern-oxygen"
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def get_embedding(text, model, dim):\n",
        "    text = text.split()\n",
        "    words = Counter(text)\n",
        "    total = len(text)\n",
        "    vectors = np.zeros((len(words), dim))\n",
        "    for i,word in enumerate(words):\n",
        "        try:\n",
        "            v = model[word]\n",
        "            vectors[i] = v*(words[word]/total)\n",
        "        except (KeyError, ValueError):\n",
        "            continue\n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    return vector"
      ],
      "id": "southern-oxygen",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brilliant-swimming",
        "outputId": "d47443a0-776d-4a0a-d2d7-7c1845d89252"
      },
      "source": [
        "dim = 50\n",
        "X_1_my_w2v = np.zeros((len(data.text_1_norm), dim))\n",
        "X_2_my_w2v = np.zeros((len(data.text_2_norm), dim))\n",
        "\n",
        "for i, text in enumerate(data.text_1_norm.values):\n",
        "    X_1_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
        "    \n",
        "for i, text in enumerate(data.text_2_norm.values):\n",
        "    X_2_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
        "\n",
        "X_my_w2v = np.concatenate([X_1_my_w2v, X_2_my_w2v], axis=1)\n",
        "y = data['label'].values\n",
        "\n",
        "dim = 50\n",
        "X_1_ready_w2v = np.zeros((len(data.text_1_norm), dim))\n",
        "X_2_ready_w2v = np.zeros((len(data.text_2_norm), dim))\n",
        "\n",
        "for i, text in enumerate(data.text_1_norm.values):\n",
        "    X_1_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
        "    \n",
        "for i, text in enumerate(data.text_2_norm.values):\n",
        "    X_2_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
        "\n",
        "X_ready_w2v = np.concatenate([X_1_ready_w2v, X_2_ready_w2v], axis=1)"
      ],
      "id": "brilliant-swimming",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hairy-poultry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ea0b2e-9ac2-4907-f9d3-b968c586ddc2"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "log_reg = LogisticRegression(C=1)\n",
        "\n",
        "print('Cross val score: ', cross_val_score(log_reg, X_my_w2v, y, scoring=\"f1_micro\"))\n",
        "print('Cross val score: ', cross_val_score(log_reg, X_ready_w2v, y, scoring=\"f1_micro\"))"
      ],
      "id": "hairy-poultry",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross val score:  [0.45228216 0.4626556  0.47404844 0.3916955  0.42906574]\n",
            "Cross val score:  [0.40940526 0.40940526 0.40899654 0.40899654 0.40899654]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prompt-fighter"
      },
      "source": [
        "Cross val score:  [0.45228216 0.4626556  0.47404844 0.3916955  0.42906574]\n",
        "Cross val score:  [0.40940526 0.40940526 0.40899654 0.40899654 0.40899654]\n",
        "\n",
        "В эффективности работы моделей нет больших различий"
      ],
      "id": "prompt-fighter"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyGJwVlQfOHn"
      },
      "source": [
        "## Задание 2 (7 баллов)\n",
        "\n",
        "Преобразуйте тексты в векторы в каждой паре 5 методами - SVD, NMF, Word2Vec (свой и русвекторовский), Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары).  \n",
        "\n",
        "Постройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру). Попробуйте улучить метрику, изменив параметры в методах векторизации. !!УТОЧНЕНИЕ: модель нужно обучить сразу на всех 5 близостях, а не по 1 модели на каждой близости!  \n",
        "\n",
        "SVD и NMF применяйте к данным напрямую, а w2w и fastext обучите на отдельном корпусе (как в первой части)."
      ],
      "id": "hyGJwVlQfOHn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tough-cambodia",
        "outputId": "4b5268c8-d542-4c6c-c6f2-68d81cc85575"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import gensim\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
        "tfidf.fit(pd.concat([data.text_1_norm, data.text_2_norm]))\n",
        "\n",
        "svd = TruncatedSVD(100)\n",
        "X_1_svd = svd.fit_transform(tfidf.transform(data.text_1_norm))\n",
        "X_2_svd = svd.fit_transform(tfidf.transform(data.text_2_norm))\n",
        "X_svd = np.concatenate([X_1_svd, X_2_svd], axis=1)\n",
        "\n",
        "nmf = NMF(100)\n",
        "X_1_nmf = nmf.fit_transform(tfidf.transform(data.text_1_norm))\n",
        "X_2_nmf = nmf.fit_transform(tfidf.transform(data.text_2_norm))\n",
        "X_nmf = np.concatenate([X_1_nmf, X_2_nmf], axis=1)\n",
        "\n",
        "fast_text = gensim.models.FastText([text.split() for text in data_lenta], size=100, min_n=4, max_n=8) \n",
        "X_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_1_ft[i] = get_embedding(text, fast_text, dim)\n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_2_ft[i] = get_embedding(text, fast_text, dim)\n",
        "X_ft = np.concatenate([X_1_ft, X_2_ft], axis=1)"
      ],
      "id": "tough-cambodia",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prospective-inventory",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34181b30-6b83-4453-b472-6e100c884a8c"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "my_w2v_dist = cosine_distances(X_1_my_w2v, X_2_my_w2v)\n",
        "ready_w2v_dist = cosine_distances(X_1_ready_w2v, X_2_ready_w2v)\n",
        "svd_dist = cosine_distances(X_1_svd, X_2_svd)\n",
        "nmf_dist = cosine_distances(X_1_nmf, X_2_nmf)\n",
        "ft_dist = cosine_distances(X_1_ft, X_2_ft)\n",
        "\n",
        "distances = np.concatenate((my_w2v_dist, ready_w2v_dist, svd_dist, nmf_dist, ft_dist), axis=1)\n",
        "\n",
        "log_reg2 = LogisticRegression(C=1, class_weight='balanced')\n",
        "\n",
        "print('Cross val score: ', cross_val_score(log_reg2, distances, y, scoring=\"f1_micro\"))"
      ],
      "id": "prospective-inventory",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross val score:  [0.37136929 0.37759336 0.36816609 0.32525952 0.37647059]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "commercial-store"
      },
      "source": [
        "Cross val score:  [0.37136929 0.37759336 0.36816609 0.32525952 0.37647059]\n",
        "\n",
        "Результат даже ухудшился. Но я изначально планировал протестировать, что будет если у tfidf выставить 1000 features, а у всех остальных размер эмбеддинга 100"
      ],
      "id": "commercial-store"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "educated-second"
      },
      "source": [
        "#### Изменим параметры"
      ],
      "id": "educated-second"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "streaming-leonard",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac384751-3dc8-45f8-96d9-22b7dec88041"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import gensim\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=5, max_df=0.9, max_features=2000)\n",
        "tfidf.fit(pd.concat([data.text_1_norm, data.text_2_norm]))\n",
        "\n",
        "svd = TruncatedSVD(200)\n",
        "X_1_svd = svd.fit_transform(tfidf.transform(data.text_1_norm))\n",
        "X_2_svd = svd.fit_transform(tfidf.transform(data.text_2_norm))\n",
        "X_svd = np.concatenate([X_1_svd, X_2_svd], axis=1)\n",
        "\n",
        "\n",
        "nmf = NMF(200)\n",
        "X_1_nmf = nmf.fit_transform(tfidf.transform(data.text_1_norm))\n",
        "X_2_nmf = nmf.fit_transform(tfidf.transform(data.text_2_norm))\n",
        "X_nmf = np.concatenate([X_1_nmf, X_2_nmf], axis=1)\n",
        "\n",
        "fast_text = gensim.models.FastText([text.split() for text in data_lenta], size=50, min_n=4, max_n=8) \n",
        "dim = 200\n",
        "X_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_1_ft[i] = get_embedding(text, fast_text, dim)\n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_2_ft[i] = get_embedding(text, fast_text, dim)\n",
        "X_ft = np.concatenate([X_1_ft, X_2_ft], axis=1)\n",
        "\n",
        "dim = 200\n",
        "X_1_my_w2v = np.zeros((len(data.text_1_norm), dim))\n",
        "X_2_my_w2v = np.zeros((len(data.text_2_norm), dim))\n",
        "for i, text in enumerate(data.text_1_norm.values):\n",
        "    X_1_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
        "for i, text in enumerate(data.text_2_norm.values):\n",
        "    X_2_my_w2v[i] = get_embedding(text, my_w2v, dim)\n",
        "X_my_w2v = np.concatenate([X_1_my_w2v, X_2_my_w2v], axis=1)\n",
        "\n",
        "dim = 200\n",
        "X_1_ready_w2v = np.zeros((len(data.text_1_norm), dim))\n",
        "X_2_ready_w2v = np.zeros((len(data.text_2_norm), dim))\n",
        "for i, text in enumerate(data.text_1_norm.values):\n",
        "    X_1_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
        "for i, text in enumerate(data.text_2_norm.values):\n",
        "    X_2_ready_w2v[i] = get_embedding(text, ready_w2v, dim)\n",
        "X_ready_w2v = np.concatenate([X_1_ready_w2v, X_2_ready_w2v], axis=1)"
      ],
      "id": "streaming-leonard",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thorough-recovery"
      },
      "source": [
        "distances = np.concatenate((my_w2v_dist, ready_w2v_dist, svd_dist, nmf_dist, ft_dist), axis=1)"
      ],
      "id": "thorough-recovery",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moderate-baseline"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg3 = LogisticRegression(C=1, class_weight='balanced')"
      ],
      "id": "moderate-baseline",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brave-kuwait",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553ad290-0a86-4b95-af9b-fde1b8e9b3aa"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "print('Cross val score: ', cross_val_score(log_reg3, distances, y, scoring=\"f1_micro\"))"
      ],
      "id": "brave-kuwait",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cross val score:  [0.37136929 0.37759336 0.36816609 0.32525952 0.37647059]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "breathing-dictionary"
      },
      "source": [
        "Результат опять не стал лучше. В дальнейших экспериментах надо увеличивать параметры"
      ],
      "id": "breathing-dictionary"
    }
  ]
}